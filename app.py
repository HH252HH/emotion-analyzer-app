# ==========================================================
# ğŸ§  Emotion AI Pro â€” NeuroVision (Enterprise + Auto Language)
# ğŸ¨ Radical UI + Stable Fixes + AR/EN Support
# ==========================================================

# ==============================
# ğŸ“¦ Imports (UNCHANGED + langdetect)
# ==============================
import streamlit as st
import requests
import os
from dotenv import load_dotenv
from PIL import Image
import plotly.graph_objects as go
import assemblyai as aai
from openai import OpenAI
from io import BytesIO
import tempfile
from langdetect import detect

# ==========================================================
# ğŸ” API Keys
# ==========================================================
load_dotenv()

HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
ASSEMBLYAI_API_KEY = os.getenv("ASSEMBLYAI_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

missing_keys = []

if not HUGGINGFACE_API_KEY:
    missing_keys.append("HUGGINGFACE_API_KEY")

if not ASSEMBLYAI_API_KEY:
    missing_keys.append("ASSEMBLYAI_API_KEY")

if not OPENAI_API_KEY:
    missing_keys.append("OPENAI_API_KEY")

if missing_keys:
    st.error(f"âŒ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù†Ø§Ù‚ØµØ©: {', '.join(missing_keys)}")
    st.info("ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡Ø§ ÙÙŠ Environment Variables ÙÙŠ Ù…Ù†ØµØ© Ø§Ù„Ù†Ø´Ø±")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# ==========================================================
# ğŸ¨ Page Config
# ==========================================================
st.set_page_config(
    page_title="Emotion AI Pro â€” NeuroVision",
    page_icon="ğŸ§ ",
    layout="wide"
)

# ==========================================================
# ğŸŒŒ UI Design
# ==========================================================
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;600;800&display=swap');

html, body, [class*="css"] {
    direction: rtl;
    font-family: 'Tajawal', sans-serif;
}

.stApp {
    background: radial-gradient(circle at 20% 30%, #1a1a2e, #0f3460, #16213e);
    background-size: 200% 200%;
    animation: moveBG 12s ease infinite;
}

@keyframes moveBG {
    0% {background-position: 0% 50%;}
    50% {background-position: 100% 50%;}
    100% {background-position: 0% 50%;}
}

.neo-card {
    background: rgba(255,255,255,0.05);
    border-radius: 25px;
    padding: 30px;
    backdrop-filter: blur(20px);
    border: 1px solid rgba(255,255,255,0.1);
    box-shadow: 0 8px 40px rgba(0,0,0,0.4);
    transition: 0.4s ease;
}

.neo-card:hover {
    transform: translateY(-6px);
    box-shadow: 0 20px 60px rgba(0,0,0,0.6);
}

.stButton>button {
    background: linear-gradient(135deg,#00c6ff,#0072ff);
    color:white;
    border:none;
    padding:14px 35px;
    border-radius:50px;
    font-size:18px;
    font-weight:600;
    transition:0.3s;
}

.stButton>button:hover {
    transform:scale(1.07);
    box-shadow:0 0 25px #00c6ff;
}

h1 {
    text-align:center;
    font-size:40px;
    font-weight:800;
    margin-bottom:30px;
}
</style>
""", unsafe_allow_html=True)

# ==========================================================
# ğŸ§  HEADER
# ==========================================================
st.title("ğŸ§  Emotion AI Pro â€” NeuroVision")

# ==========================================================
# ğŸ“¥ INPUT SECTION
# ==========================================================
col1, col2 = st.columns(2)

with col1:
    st.markdown("<div class='neo-card'>", unsafe_allow_html=True)
    st.subheader("ğŸ“· ØªØ­Ù„ÙŠÙ„ ØªØ¹Ø§Ø¨ÙŠØ± Ø§Ù„ÙˆØ¬Ù‡")

    image_option = st.radio(
        "Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„ØµÙˆØ±Ø©:",
        ["Ø±ÙØ¹ ØµÙˆØ±Ø©", "Ø§Ù„ØªÙ‚Ø§Ø· Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"],
        horizontal=True
    )

    image_bytes = None
    if image_option == "Ø±ÙØ¹ ØµÙˆØ±Ø©":
        uploaded_img = st.file_uploader("Ø§Ø®ØªØ± ØµÙˆØ±Ø©", type=["jpg", "png"])
        if uploaded_img:
            image_bytes = uploaded_img.getvalue()
    else:
        camera_img = st.camera_input("Ø§Ù„ØªÙ‚Ø· ØµÙˆØ±Ø©")
        if camera_img:
            image_bytes = camera_img.getvalue()

    st.markdown("</div>", unsafe_allow_html=True)

with col2:
    st.markdown("<div class='neo-card'>", unsafe_allow_html=True)
    st.subheader("ğŸ¤ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„ØµÙˆØªÙŠØ©")

    audio_option = st.radio(
        "Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„ØµÙˆØª:",
        ["Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙŠ", "ØªØ³Ø¬ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†"],
        horizontal=True
    )

    audio_bytes = None
    if audio_option == "Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙŠ":
        uploaded_audio = st.file_uploader(
            "Ø§Ø®ØªØ± ØµÙˆØª",
            type=["mp3", "wav", "m4a"]
        )
        if uploaded_audio:
            audio_bytes = uploaded_audio.getvalue()
    else:
        recorded_audio = st.audio_input("Ø³Ø¬Ù„ ØµÙˆØªÙƒ")
        if recorded_audio:
            audio_bytes = recorded_audio.getvalue()

    st.markdown("</div>", unsafe_allow_html=True)

# ==========================================================
# ğŸ“· IMAGE ANALYSIS
# ==========================================================
def analyze_image(image_bytes):
    try:
        API_URL = "https://router.huggingface.co/hf-inference/models/trpakov/vit-face-expression"
        headers = {
            "Authorization": f"Bearer {HUGGINGFACE_API_KEY}",
            "Content-Type": "application/octet-stream"
        }
        response = requests.post(API_URL, headers=headers, data=image_bytes, timeout=60)
        response.raise_for_status()
        result = response.json()

        if not isinstance(result, list) or len(result) == 0:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ¬Ù‡.")
            return None, None

        dominant = max(result, key=lambda x: x['score'])
        return result, dominant

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£: {e}")
        return None, None

# ==========================================================
# ğŸ¤ AUDIO ANALYSIS
# ==========================================================
def analyze_audio(audio_bytes):
    try:
        aai.settings.api_key = ASSEMBLYAI_API_KEY

        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            tmp_path = tmp.name

        config = aai.TranscriptionConfig(
            speech_models=["universal"],
            sentiment_analysis=True
        )

        transcript = aai.Transcriber().transcribe(tmp_path, config=config)
        os.remove(tmp_path)

        if transcript.status == aai.TranscriptStatus.error:
            st.error(f"âŒ ÙØ´Ù„ Ø§Ù„ØªÙØ±ÙŠØº: {transcript.error}")
            return None, None

        return transcript.text or "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ.", transcript.sentiment_analysis or []

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØª: {e}")
        return None, None

# ==========================================================
# ğŸŒ FINAL EMOTIONAL DIAGNOSIS (UPDATED ONLY HERE)
# ==========================================================
def generate_diagnosis(image_emotion, audio_text):
    try:
        try:
            detected_lang = detect(audio_text) if audio_text.strip() else "ar"
        except:
            detected_lang = "ar"

        if detected_lang == "en":
            system_msg = "You are a professional emotional and psychological AI analyst."

            final_prompt = f"""
            Face emotion analysis: {image_emotion}
            Speech content: {audio_text}

            Provide a unified emotional diagnosis by combining facial expression and speech analysis.

            Your response must include:
            1) Clear explanation of the person's emotional state.
            2) Interpretation of psychological condition.
            3) If emotional state is negative â†’ provide practical steps to improve mood and mental state.
            4) If emotional state is positive â†’ provide advice to maintain and strengthen well-being.

            Response must be clear, supportive, and professional.
            """

        else:
            system_msg = "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù†ÙØ³ÙŠ ÙˆØ¹Ø§Ø·ÙÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ."

            final_prompt = f"""
            ØªØ­Ù„ÙŠÙ„ ØªØ¹Ø§Ø¨ÙŠØ± Ø§Ù„ÙˆØ¬Ù‡: {image_emotion}
            Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {audio_text}

            Ù‚Ø¯Ù… ØªØ´Ø®ÙŠØµÙ‹Ø§ Ø¹Ø§Ø·ÙÙŠÙ‹Ø§ Ù…ÙˆØ­Ø¯Ù‹Ø§ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„ØµÙˆØª.

            ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ¶Ù…Ù† Ø§Ù„Ø±Ø¯:
            1) Ø´Ø±Ø­ ÙˆØ§Ø¶Ø­ Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ù„Ù„Ø´Ø®Øµ.
            2) ØªÙØ³ÙŠØ± Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©.
            3) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ø§Ù„Ø© Ø³Ù„Ø¨ÙŠØ© â†’ Ù‚Ø¯Ù… Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø²Ø§Ø¬ ÙˆØ§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©.
            4) Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ø§Ù„Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© â†’ Ù‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØªØ¹Ø²ÙŠØ² Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¬ÙŠØ¯Ø©.

            Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ Ø¯Ø§Ø¹Ù…Ù‹Ø§ ÙˆÙˆØ§Ø¶Ø­Ù‹Ø§ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§.
            """

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": final_prompt}
            ],
            temperature=0.7,
            max_tokens=800
        )

        return response.choices[0].message.content, detected_lang

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ OpenAI: {e}")
        return None, None

# ==========================================================
# ğŸš€ START ANALYSIS
# ==========================================================
if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ"):

    if image_bytes and audio_bytes:

        progress = st.progress(0)

        progress.progress(20)
        emotions, dominant = analyze_image(image_bytes)
        if not emotions:
            st.stop()

        progress.progress(50)
        text, sentiments = analyze_audio(audio_bytes)
        if text is None:
            st.stop()

        progress.progress(70)
        diagnosis, lang = generate_diagnosis(dominant["label"], text)

        progress.progress(100)

        st.markdown("<div class='neo-card'>", unsafe_allow_html=True)
        colA, colB = st.columns(2)

        with colA:
            st.image(Image.open(BytesIO(image_bytes)), use_container_width=True)

        with colB:
            labels = [e["label"] for e in emotions]
            scores = [e["score"] for e in emotions]

            fig = go.Figure([go.Bar(x=labels, y=scores)])
            fig.update_layout(
                title="ØªØ­Ù„ÙŠÙ„ ØªØ¹Ø§Ø¨ÙŠØ± Ø§Ù„ÙˆØ¬Ù‡",
                template="plotly_dark"
            )
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='neo-card'>", unsafe_allow_html=True)
        st.subheader("ğŸ“Š Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        st.info(f"ğŸŒ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang == 'ar' else 'English'}")
        st.write(diagnosis)
        st.markdown("</div>", unsafe_allow_html=True)

    else:
        st.warning("âš  ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ØµÙˆØ±Ø© ÙˆØµÙˆØª Ø£ÙˆÙ„Ø§Ù‹.")